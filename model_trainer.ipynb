{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 20:26:52.117436: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-18 20:26:52.134529: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-18 20:26:52.139871: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-18 20:26:52.153306: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras:  3.6.0\n",
      "tensorflow:  2.17.0\n",
      "python:  3.10.13 | packaged by conda-forge | (main, Dec 23 2023, 15:36:39) [GCC 12.3.0]\n",
      "unknown 2.17.0\n",
      "Num GPUs Available:  3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "os.environ['XLA_FLAGS'] = '--xla_gpu_strict_conv_algorithm_picker=false'\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import ops\n",
    "from keras import layers\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras import utils, models\n",
    "from keras.layers import Rescaling\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "\n",
    "print('keras: ', keras.__version__)\n",
    "print('tensorflow: ',tf.__version__)\n",
    "print('python: ',sys.version)\n",
    "print(tf.version.GIT_VERSION, tf.version.VERSION)\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 20:26:54.753770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 43611 MB memory:  -> device: 0, name: NVIDIA A40, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2025-03-18 20:26:54.755538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 43611 MB memory:  -> device: 1, name: NVIDIA A40, pci bus id: 0000:c1:00.0, compute capability: 8.6\n",
      "2025-03-18 20:26:54.757260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 43611 MB memory:  -> device: 2, name: NVIDIA A40, pci bus id: 0000:e1:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 3\n"
     ]
    }
   ],
   "source": [
    "## CONFIGURATII\n",
    "\n",
    "epochs = 100\n",
    "latent_dim = 512\n",
    "height = 128\n",
    "width = 128\n",
    "no_images = 192469\n",
    "batch_size = 192\n",
    "test_image_dir = './Dataset/test'\n",
    "test_image_name = '000206.jpg'\n",
    "\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a face.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.seed_generator = keras.random.SeedGenerator(280602)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = ops.shape(z_mean)[0]\n",
    "        dim = ops.shape(z_mean)[1]\n",
    "        epsilon = keras.random.normal(shape=(batch, dim), seed=self.seed_generator)\n",
    "        return z_mean + ops.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder(input_shape=(128, 128, 3), latent_dim=256):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    \n",
    "    \n",
    "    x = layers.Conv2D(64, (4,4), activation='leaky_relu', strides=2, padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = layers.Conv2D(128, (4,4), activation='leaky_relu', strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = layers.Conv2D(256, (4,4), activation='leaky_relu', strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = layers.Conv2D(512, (4,4), activation='leaky_relu', strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = layers.Flatten()(x)\n",
    "    \n",
    "    # Add dense layers before final latent space\n",
    "    x = layers.Dense(512, activation='leaky_relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    \n",
    "    return keras.Model(inputs, [z_mean, z_log_var, z], name='improved_encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_decoder(latent_dim=256):\n",
    "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "    \n",
    "    x = layers.Dense(8*8*512, activation='leaky_relu')(latent_inputs)\n",
    "    x = layers.Reshape((8, 8, 512))(x)\n",
    "    \n",
    "    x = layers.Conv2DTranspose(256, (4,4), activation='leaky_relu', strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = layers.Conv2DTranspose(128, (4,4), activation='leaky_relu', strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = layers.Conv2DTranspose(64, (4,4), activation='leaky_relu', strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = layers.Conv2DTranspose(32, (4,4), activation='leaky_relu', strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    decoder_outputs = layers.Conv2DTranspose(3, (4,4), activation='sigmoid', padding='same')(x)\n",
    "    \n",
    "    return keras.Model(latent_inputs, decoder_outputs, name='improved_decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        # Initialize VGG19 for feature extraction (use pre-trained weights)\n",
    "        vgg = VGG19(include_top=False, weights='imagenet', input_shape=(None, None, 3))\n",
    "        \n",
    "        # Choose intermediate layers for feature comparison\n",
    "        self.feature_layers = [\n",
    "            'block1_conv2',  # Low-level features\n",
    "            'block2_conv2',  # Mid-level features\n",
    "            'block3_conv2',  # Higher-level features\n",
    "            'block4_conv2'   # Very high-level features\n",
    "        ]\n",
    "        \n",
    "        # Create a model that outputs features from these layers\n",
    "        self.feature_extractor = keras.Model(\n",
    "            inputs=vgg.input, \n",
    "            outputs=[vgg.get_layer(name).output for name in self.feature_layers]\n",
    "        )\n",
    "        \n",
    "        # Freeze the VGG19 weights\n",
    "        self.feature_extractor.trainable = False\n",
    "        \n",
    "        # Tracking metrics\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name='total_loss')\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name='reconstruction_loss')\n",
    "        self.perceptual_loss_tracker = keras.metrics.Mean(name='perceptual_loss')\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name='kl_loss')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Implement the forward pass\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstruction = self.decoder(z)\n",
    "        return reconstruction\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.perceptual_loss_tracker,\n",
    "            self.kl_loss_tracker \n",
    "        ]\n",
    "    \n",
    "    def compute_perceptual_loss(self, original, reconstructed):\n",
    "        # Preprocess images for VGG19 (ensure 3 channels and correct scaling)\n",
    "        original_processed = preprocess_input(original * 255.0)\n",
    "        reconstructed_processed = preprocess_input(reconstructed * 255.0)\n",
    "        \n",
    "        # Extract features for original and reconstructed images\n",
    "        original_features = self.feature_extractor(original_processed)\n",
    "        reconstructed_features = self.feature_extractor(reconstructed_processed)\n",
    "        \n",
    "        # Compute perceptual loss as mean squared error between features\n",
    "        perceptual_loss = 0\n",
    "        for orig_feat, recon_feat in zip(original_features, reconstructed_features):\n",
    "            perceptual_loss += ops.mean(ops.square(orig_feat - recon_feat))\n",
    "        \n",
    "        # Normalize by the number of feature layers\n",
    "        perceptual_loss /= len(self.feature_layers)\n",
    "        \n",
    "        return perceptual_loss\n",
    "    \n",
    "    def compute_reconstruction_loss(self, original, reconstructed):\n",
    "        \n",
    "        reconstruction_loss = ops.mean(\n",
    "                ops.sum(\n",
    "                    keras.losses.binary_crossentropy(original, reconstructed),\n",
    "                    axis=(1,2)\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        return reconstruction_loss\n",
    "   \n",
    "    \n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Encoder forward pass\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            \n",
    "            # Decoder reconstruction\n",
    "            reconstruction = self.decoder(z)\n",
    "            \n",
    "            # Compute KL divergence loss\n",
    "            kl_loss = -0.5 * (1 + z_log_var - ops.square(z_mean) - ops.exp(z_log_var))\n",
    "            kl_loss = ops.mean(ops.sum(kl_loss, axis=1))\n",
    "            \n",
    "            # Compute perceptual loss using VGG19 features\n",
    "            perceptual_loss = self.compute_perceptual_loss(data, reconstruction)\n",
    "            \n",
    "            reconstruction_loss = self.compute_reconstruction_loss(data,reconstruction)\n",
    "            \n",
    "            # Total loss combines perceptual loss and KL divergence\n",
    "            total_loss = perceptual_loss + kl_loss + reconstruction_loss\n",
    "        \n",
    "        # Compute gradients and apply them\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        \n",
    "        # Update metrics\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.perceptual_loss_tracker.update_state(perceptual_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"perceptual_loss\": self.perceptual_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "            \"recreation_loss\": self.reconstruction_loss_tracker.result()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 192469 files.\n"
     ]
    }
   ],
   "source": [
    "train_data = utils.image_dataset_from_directory(\n",
    "    './Dataset/train',\n",
    "    labels=None,\n",
    "    label_mode='categorical',\n",
    "    batch_size=batch_size,\n",
    "    image_size=(128, 128),\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    interpolation='bilinear',\n",
    ")\n",
    "\n",
    "# Normalize the images by dividing by 255.0\n",
    "normalization_layer = Rescaling(1./255)\n",
    "train_data = train_data.map(lambda x: normalization_layer(x))\n",
    "\n",
    "#train_data = train_data.batch(batch_size=batch_size,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CustomModelCheckpoint(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, save_freq, filepath):\n",
    "        super(CustomModelCheckpoint, self).__init__()\n",
    "        self.save_freq = save_freq\n",
    "        self.filepath = filepath\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % self.save_freq == 0:\n",
    "            self.model.save_weights(self.filepath.format(epoch=epoch + 1))\n",
    "\n",
    "with strategy.scope():\n",
    "    encoder = create_encoder(latent_dim=latent_dim)\n",
    "    decoder = create_decoder(latent_dim=latent_dim)\n",
    "\n",
    "    # Compile with better optimizer\n",
    "    vae = VAE(encoder, decoder)\n",
    "    vae.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4))\n",
    "\n",
    "    vae.encoder.summary()\n",
    "    vae.decoder.summary()\n",
    "    vae.summary()\n",
    "\n",
    "    load = False\n",
    "    run = True\n",
    "    if run == True:\n",
    "        if load == True:\n",
    "            vae.load_weights('results_encode_decode/1/vae_weights.weights.h5')\n",
    "        else:\n",
    "            # Define the callbacks\n",
    "            checkpoint_callback = CustomModelCheckpoint(save_freq=10, filepath='results_encode_decode/last/vae_weights_epoch_{epoch}.h5')\n",
    "            csv_logger = CSVLogger('training_log.csv', append=True)\n",
    "\n",
    "            # Fit the model with callbacks\n",
    "            vae.fit(train_data, epochs=epochs, callbacks=[checkpoint_callback, csv_logger])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "img = Image.open(os.path.join(test_image_dir, test_image_name))\n",
    "img = img.resize((height, width))\n",
    "img = np.array(img)\n",
    "img = img.astype('float32') / 255.\n",
    "\n",
    "\n",
    "z_mean, z_log_var, z = encoder.predict(np.expand_dims(img, axis=0))\n",
    "\n",
    "reconstructed_img = decoder.predict(z)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "\n",
    "axes[0].imshow(img)\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(reconstructed_img[0])\n",
    "axes[1].set_title('Reconstructed Image')\n",
    "axes[1].axis('off')\n",
    "\n",
    "\n",
    "\n",
    "# Add information about the run to the plot\n",
    "info_text = f\"Latent Space: {latent_dim} Epochs:{epochs}\\nImage Size: {height}x{width}\\nNumber of Images for Training: {no_images}\\n\"\n",
    "fig.text(0.5, 0.01, info_text, ha='center',fontsize=15, bbox=dict(facecolor='red', alpha=0.5))\n",
    "\n",
    "SAVE = True\n",
    "if SAVE:\n",
    "    # Save the plot as a JPG file in the specified folder\n",
    "    output_dir = 'results_encode_decode'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    result_counter_path = os.path.join(output_dir, 'result_counter.txt')\n",
    "\n",
    "    if os.path.exists(result_counter_path):\n",
    "        with open(result_counter_path, 'r') as file:\n",
    "            result_no = int(file.read().strip()) + 1\n",
    "    else:\n",
    "        result_no = 0\n",
    "\n",
    "    with open(result_counter_path, 'w') as file:\n",
    "        file.write(str(result_no))\n",
    "\n",
    "    result_dir = os.path.join(output_dir, str(result_no))\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "    # Save weights in result dir\n",
    "    vae.save_weights(os.path.join(result_dir, 'vae_weights.weights.h5'))\n",
    "    vae.encoder.save_weights(os.path.join(result_dir, 'encoder_weights.weights.h5'))\n",
    "    vae.decoder.save_weights(os.path.join(result_dir, 'decoder_weights.weights.h5'))\n",
    "\n",
    "    output_path = os.path.join(result_dir, 'result.jpg')\n",
    "    plt.savefig(output_path, format='jpg')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proiect_diploma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
