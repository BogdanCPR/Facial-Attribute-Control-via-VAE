{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d045a3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 18:20:36.439462: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-25 18:20:36.456719: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-25 18:20:36.462102: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-25 18:20:36.475839: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "os.environ['XLA_FLAGS'] = '--xla_gpu_strict_conv_algorithm_picker=false'\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import ops\n",
    "from keras import layers\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras import utils, models\n",
    "from keras.layers import Rescaling\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "\n",
    "#####CONFIGS#####\n",
    "Train = True\n",
    "epochs = 100\n",
    "latent_dim = 512\n",
    "BATCH_SIZE_PER_REPLICA = 64\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "602f1e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a face.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.seed_generator = keras.random.SeedGenerator(280602)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = ops.shape(z_mean)[0]\n",
    "        dim = ops.shape(z_mean)[1]\n",
    "        epsilon = keras.random.normal(shape=(batch, dim), seed=self.seed_generator)\n",
    "        return z_mean + ops.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "def create_encoder(input_shape=(128, 128, 3), latent_dim=512):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    \n",
    "    \n",
    "    x = layers.Conv2D(64, (4,4), activation='leaky_relu', strides=2, padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = layers.Conv2D(128, (4,4), activation='leaky_relu', strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = layers.Conv2D(256, (4,4), activation='leaky_relu', strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = layers.Conv2D(512, (4,4), activation='leaky_relu', strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = layers.Flatten()(x)\n",
    "    \n",
    "    # Add dense layers before final latent space\n",
    "    x = layers.Dense(512, activation='leaky_relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    \n",
    "    return keras.Model(inputs, [z_mean, z_log_var, z], name='improved_encoder')\n",
    "\n",
    "\n",
    "def create_decoder(latent_dim=512):\n",
    "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "    \n",
    "    x = layers.Dense(8*8*512, activation='leaky_relu')(latent_inputs)\n",
    "    x = layers.Reshape((8, 8, 512))(x)\n",
    "    \n",
    "    x = layers.Conv2DTranspose(256, (4,4), activation='leaky_relu', strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = layers.Conv2DTranspose(128, (4,4), activation='leaky_relu', strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = layers.Conv2DTranspose(64, (4,4), activation='leaky_relu', strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = layers.Conv2DTranspose(32, (4,4), activation='leaky_relu', strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    decoder_outputs = layers.Conv2DTranspose(3, (4,4), activation='sigmoid', padding='same')(x)\n",
    "    \n",
    "    return keras.Model(latent_inputs, decoder_outputs, name='improved_decoder')\n",
    "\n",
    "\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.lambda_reconstruction = 0.1\n",
    "        self.lambda_perceptual = 0.1\n",
    "        \n",
    "        # Initialize VGG19 for feature extraction (use pre-trained weights)\n",
    "        vgg = VGG19(include_top=False, weights='imagenet', input_shape=(None, None, 3))\n",
    "        \n",
    "        # Choose intermediate layers for feature comparison\n",
    "        self.feature_layers = [\n",
    "            'block1_conv2',  # Low-level features\n",
    "            'block2_conv2',  # Mid-level features\n",
    "            'block3_conv2',  # Higher-level features\n",
    "            'block4_conv2'   # Very high-level features\n",
    "        ]\n",
    "        \n",
    "        # Create a model that outputs features from these layers\n",
    "        self.feature_extractor = keras.Model(\n",
    "            inputs=vgg.input, \n",
    "            outputs=[vgg.get_layer(name).output for name in self.feature_layers]\n",
    "        )\n",
    "        \n",
    "        # Freeze the VGG19 weights\n",
    "        self.feature_extractor.trainable = False\n",
    "        \n",
    "        # Tracking metrics\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name='total_loss')\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name='reconstruction_loss')\n",
    "        self.perceptual_loss_tracker = keras.metrics.Mean(name='perceptual_loss')\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name='kl_loss')\n",
    "        \n",
    "        # Add validation metrics\n",
    "        self.val_total_loss_tracker = keras.metrics.Mean(name='val_total_loss')\n",
    "        self.val_reconstruction_loss_tracker = keras.metrics.Mean(name='val_reconstruction_loss')\n",
    "        self.val_perceptual_loss_tracker = keras.metrics.Mean(name='val_perceptual_loss')\n",
    "        self.val_kl_loss_tracker = keras.metrics.Mean(name='val_kl_loss')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Implement the forward pass\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstruction = self.decoder(z)\n",
    "        return reconstruction\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.perceptual_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "            self.val_total_loss_tracker,\n",
    "            self.val_reconstruction_loss_tracker,\n",
    "            self.val_perceptual_loss_tracker,\n",
    "            self.val_kl_loss_tracker\n",
    "        ]\n",
    "    \n",
    "    def compute_perceptual_loss(self, original, reconstructed):\n",
    "        # Preprocess images for VGG19 (ensure 3 channels and correct scaling)\n",
    "        original_processed = preprocess_input(original * 255.0)\n",
    "        reconstructed_processed = preprocess_input(reconstructed * 255.0)\n",
    "        \n",
    "        # Extract features for original and reconstructed images\n",
    "        original_features = self.feature_extractor(original_processed)\n",
    "        reconstructed_features = self.feature_extractor(reconstructed_processed)\n",
    "    \n",
    "        \n",
    "        # Compute perceptual loss as mean squared error between features\n",
    "        perceptual_loss = 0\n",
    "        for orig_feat, recon_feat in zip(original_features, reconstructed_features):\n",
    "            perceptual_loss += ops.mean(ops.square(orig_feat - recon_feat))\n",
    "\n",
    "        \n",
    "        # Normalize by the number of feature layers\n",
    "        perceptual_loss /= len(self.feature_layers)\n",
    "        \n",
    "        return perceptual_loss\n",
    "    \n",
    "    def compute_reconstruction_loss(self, original, reconstructed):\n",
    "        \n",
    "        reconstruction_loss = ops.mean(\n",
    "                ops.sum(\n",
    "                    keras.losses.binary_crossentropy(original, reconstructed),\n",
    "                    axis=(1,2)\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        return reconstruction_loss\n",
    "   \n",
    "    \n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Encoder forward pass\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            \n",
    "            # Decoder reconstruction\n",
    "            reconstruction = self.decoder(z)\n",
    "            \n",
    "            # Compute KL divergence loss\n",
    "            kl_loss = -0.5 * (1 + z_log_var - ops.square(z_mean) - ops.exp(z_log_var))\n",
    "            kl_loss = ops.mean(ops.sum(kl_loss, axis=1))\n",
    "            \n",
    "            \n",
    "            # Compute perceptual loss using VGG19 features\n",
    "            perceptual_loss = self.compute_perceptual_loss(data, reconstruction) * self.lambda_perceptual\n",
    "            \n",
    "            reconstruction_loss = self.compute_reconstruction_loss(data, reconstruction) * self.lambda_reconstruction\n",
    "            \n",
    "            # Total loss combines reconstruction losses and KL divergence\n",
    "            total_loss = kl_loss + (perceptual_loss + reconstruction_loss)/2\n",
    "        \n",
    "        # Compute gradients and apply them\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        \n",
    "        # Update metrics\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.perceptual_loss_tracker.update_state(perceptual_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"perceptual_loss\": self.perceptual_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result()\n",
    "        }\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        # Encoder forward pass\n",
    "        z_mean, z_log_var, z = self.encoder(data)\n",
    "        \n",
    "        # Decoder reconstruction\n",
    "        reconstruction = self.decoder(z)\n",
    "        \n",
    "        # Compute KL divergence loss\n",
    "        kl_loss = -0.5 * (1 + z_log_var - ops.square(z_mean) - ops.exp(z_log_var))\n",
    "        kl_loss = ops.mean(ops.sum(kl_loss, axis=1))\n",
    "        \n",
    "        # Compute perceptual loss using VGG19 features\n",
    "        perceptual_loss = self.compute_perceptual_loss(data, reconstruction) * self.lambda_perceptual\n",
    "        \n",
    "        reconstruction_loss = self.compute_reconstruction_loss(data, reconstruction) * self.lambda_reconstruction\n",
    "        \n",
    "        # Total loss combines perceptual loss and KL divergence\n",
    "        total_loss = kl_loss + (reconstruction_loss + perceptual_loss)/2\n",
    "        \n",
    "        # Update validation metrics\n",
    "        self.val_total_loss_tracker.update_state(total_loss)\n",
    "        self.val_perceptual_loss_tracker.update_state(perceptual_loss)\n",
    "        self.val_kl_loss_tracker.update_state(kl_loss)\n",
    "        self.val_reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": self.val_total_loss_tracker.result(),\n",
    "            \"perceptual_loss\": self.val_perceptual_loss_tracker.result(),\n",
    "            \"kl_loss\": self.val_kl_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.val_reconstruction_loss_tracker.result()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1323b7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 202599 files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 18:20:45.464611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 43611 MB memory:  -> device: 0, name: NVIDIA A40, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2025-05-25 18:20:45.466621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 43611 MB memory:  -> device: 1, name: NVIDIA A40, pci bus id: 0000:c1:00.0, compute capability: 8.6\n",
      "2025-05-25 18:20:45.468261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 43611 MB memory:  -> device: 2, name: NVIDIA A40, pci bus id: 0000:e1:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "###DATA###        \n",
    "full_dataset = utils.image_dataset_from_directory(\n",
    "    './Dataset/all_images',\n",
    "    seed=123,\n",
    "    shuffle=True,\n",
    "    image_size=(128, 128),\n",
    "    batch_size=None,\n",
    "    label_mode=None\n",
    ")\n",
    "\n",
    "normalization_layer = Rescaling(1./255)\n",
    "full_dataset = full_dataset.map(lambda x: normalization_layer(x))\n",
    "\n",
    "# Compute counts\n",
    "total_images = 200_000\n",
    "train_count = 180_000\n",
    "val_count = 10_000\n",
    "test_count = 10_000 #approximately\n",
    "\n",
    "# Split manually\n",
    "train_data = full_dataset.take(train_count)\n",
    "val_test_split = full_dataset.skip(train_count)\n",
    "val_data = val_test_split.take(val_count)\n",
    "test_data = val_test_split.skip(val_count)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_data = train_data.batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_data = val_data.batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_data = test_data.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a2e611",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load weights from previous training\n",
    "encoder = create_encoder(latent_dim=latent_dim)\n",
    "decoder = create_decoder(latent_dim=latent_dim)\n",
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4))\n",
    "vae.load_weights('results_encode_decode/20/vae_weights.weights.h5')\n",
    "vae.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bff2a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate reconstructions from test data\n",
    "for test_batch in test_data.take(1):\n",
    "    sample_images = test_batch[:10]  # Take 10 sample images\n",
    "\n",
    "    # Encode and decode the samples\n",
    "    z_mean, z_log_var, z = vae.encoder(sample_images)\n",
    "    reconstructions = vae.decoder(z)\n",
    "\n",
    "    # Plot original vs reconstructed images\n",
    "    fig, axes = plt.subplots(2, 10, figsize=(20, 4))\n",
    "    for i in range(10):\n",
    "        axes[0, i].imshow(sample_images[i])\n",
    "        axes[0, i].set_title('Original')\n",
    "        axes[0, i].axis('off')\n",
    "        axes[1, i].imshow(reconstructions[i])\n",
    "        axes[1, i].set_title('Reconstructed')\n",
    "        axes[1, i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    output_dir = 'results_encode_decode/20/attribute_analysis/test_manipulations'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, 'test_reconstructions.jpg'), format='jpg', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # Use the same face for all attribute manipulations\n",
    "    img = sample_images[1:2]\n",
    "    z_mean_img, _, z_img = vae.encoder(img)\n",
    "    recon = vae.decoder(z_img)\n",
    "    \n",
    "    import glob\n",
    "    attribute_files = glob.glob('results_encode_decode/20/attribute_analysis/attribute_vectors/*')\n",
    "    \n",
    "    for attr_file in attribute_files:\n",
    "        attr_name = os.path.splitext(os.path.basename(attr_file))[0]\n",
    "        attr_vec = np.load(attr_file, allow_pickle=True)\n",
    "        attr_vec = attr_vec.reshape(1, -1)  # Ensure shape is (1, latent_dim)\n",
    "\n",
    "        # Add and subtract attribute vector\n",
    "        z_plus = z_img + 5 * attr_vec\n",
    "        z_minus = z_img - 5 * attr_vec\n",
    "        recon_plus = vae.decoder(z_plus)\n",
    "        recon_minus = vae.decoder(z_minus)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
    "        axes[0].imshow(img[0])\n",
    "        axes[0].set_title('Original')\n",
    "        axes[0].axis('off')\n",
    "        axes[1].imshow(recon[0])\n",
    "        axes[1].set_title('Reconstructed')\n",
    "        axes[1].axis('off')\n",
    "        axes[2].imshow(recon_plus[0])\n",
    "        axes[2].set_title(f'{attr_name} +')\n",
    "        axes[2].axis('off')\n",
    "        axes[3].imshow(recon_minus[0])\n",
    "        axes[3].set_title(f'{attr_name} -')\n",
    "        axes[3].axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f'{attr_name}_manipulation.jpg'), format='jpg', dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "        # Attribute traversals in latent space: coefficients from -4 to 4, skipping 0\n",
    "        coeffs = [-4, -3, -2, 0, 2, 3, 4, 5]\n",
    "        fig, axes = plt.subplots(1, len(coeffs), figsize=(2 * len(coeffs), 2))\n",
    "        for j, coeff in enumerate(coeffs):\n",
    "            z_mod = z_img + coeff * attr_vec\n",
    "            recon_mod = vae.decoder(z_mod)\n",
    "            axes[j].imshow(recon_mod[0])\n",
    "            axes[j].set_title(f'{coeff:+d}')\n",
    "            axes[j].axis('off')\n",
    "        plt.suptitle(f'Latent Traversal: {attr_name}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f'{attr_name}_traversal.jpg'), format='jpg', dpi=300)\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proiect_diploma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
